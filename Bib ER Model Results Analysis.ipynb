{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The journey from sucky precision and recall to not sucky \n",
    "\n",
    "Model 1:\n",
    "\n",
    "40: ( HaveSimilarTitles(P1,P2) & BlocksPubs(P1,B) & BlocksPubs(P2,B) & (P1-P2) ) >> SamePub(P1,P2) ^ 2  \n",
    "[\"SimilarTitles\"]\n",
    "\n",
    "40: ( HaveSimilarNames(A1,A2) & BlocksAuthors(A1,B) & BlocksAuthors(A2,B) & (A1-A2) ) >> SameAuthor(A1,A2) ^ 2  \n",
    "[\"SimilarNames\"]\n",
    "\n",
    "40: ( ~HaveSimilarAuthors(P1,P2) & BlocksPubs(P1,B) & BlocksPubs(P2,B) & (P1-P2) ) >> ~SamePub(P1,P2) ^ 2  \n",
    "[\"NotSimilarAuthors\"]\n",
    "\n",
    "20: ( SamePub(P1,P2) & HasAuthor(P1,A1) & HasAuthor(P2,A2) & HaveSimilarNames(A1,A2) & BlocksPubs(P1,PB) & BlocksPubs(P2,PB) & BlocksAuthors(A1,AB) & BlocksAuthors(A2,AB) & (P1-P2) & (A1-A2)) >> SameAuthor(A1,A2) ^ 2  \n",
    "[\"SamePubSameAuthor\"]\n",
    "\n",
    "20: ( AreCoAuthors(A1,A2) & AreCoAuthors(A3,A4) & SameAuthor(A1,A3) & HaveSimilarNames(A2,A4) & BlocksAuthors(A1,B) & BlocksAuthors(A2,B) & BlocksAuthors(A3,B) & BlocksAuthors(A4,B) & (A1-A2) & (A3-A4) & (A1-A3) & (A2-A4)) >> SameAuthor(A2,A4) ^ 2  \n",
    "[\"Co-occurrence\"]\n",
    "\n",
    "1: ( (P1-P2) & BlocksPubs(P1,B) & BlocksPubs(P2,B)) >> ~SamePub(P1,P2) ^ 2  \n",
    "[\"Prior\"]\n",
    "\n",
    "1: ( (A1-A2) & BlocksAuthors(A1,B) & BlocksAuthors(A2,B)) >> ~SameAuthor(A1,A2) ^ 2  \n",
    "[\"Prior\"]\n",
    "\n",
    "```\n",
    "Stats for Author\n",
    "Positive Class: P 0.0806, R 0.8138\n",
    "Negative Class: P 0.2527, R 0.0067\n",
    "\n",
    "Stats for Publications\n",
    "Positive Class: P 0.0022, R 0.1099\n",
    "Negative Class: P 0.8532, R 0.0938\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Did you add explicit zero truths to target variables? - Eriq\n",
    "\n",
    "#### :/ Now I did.\n",
    "\n",
    "```\n",
    "Stats for Author\n",
    "Positive Class: P 0.8577, R 0.8139\n",
    "Negative Class: P 0.9797, R 0.9852\n",
    "\n",
    "Positive Class: P 0.1781, R 0.8579\n",
    "Negative Class: P 0.9972, recall 0.9283\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Didn't transitivity help?\n",
    "\n",
    "Model 2:\n",
    "\n",
    "Model 1 with\n",
    "\n",
    "40: ( SamePub(P1,P2) & SamePub(P2,P3) & (P1-P3) & BlocksPubs(P1,B) & BlocksPubs(P2,B) & BlocksPubs(P3,B)) >> SamePub(P1,P3) ^ 2  \n",
    "[\"Transitivity Publications\"]\n",
    "\n",
    "40: ( SameAuthor(A1,A2) & SameAuthor(A2,A3) & (A1-A3) & BlocksAuthors(A1,B) & BlocksAuthors(A2,B) & BlocksAuthors(A3,B)) >> SameAuthor(A1,A3) ^ 2  \n",
    "[\"Transitivity Authors\"]\n",
    "\n",
    "```\n",
    "Stats for Author\n",
    "Positive Class: P 0.8574, R 0.9442\n",
    "Negative Class: P 0.9938, R 0.9828\n",
    "\n",
    "Stats for Publications\n",
    "Positive Class: P 0.1871, R 0.8579\n",
    "Negative Class: P 0.9973, R 0.9325\n",
    "```\n",
    "\n",
    "#### Not by much. Most significantly, recall for authors improved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What about the threshold? What does the inferred values look like?\n",
    "\n",
    "Well, first off, it's low precision, so it can't be that the threshold is *low*. And indeed it isn't. Of 9900 publication pairs, there are 176 positive pairs, and our inference has 906 pairs with non-zero truth values, most of them over 0.5. More than the threshold being low, looks like we should be increasing the negative prior weight?\n",
    "\n",
    "The current weight seems to work decently for authors though..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It's the similarity metric isn't it?\n",
    "\n",
    "One look at the rules and you realize that \"wait a minute, there's only one rule that gives any evidence towards a positive same publication\", the similar titles rule. And one glance at the file shows that it gives out scores very generously...\n",
    "\n",
    "Well for one thing, the library (pyjarowinkler) has incorrectly named functions. If it's a distance measure, dist(a, a) should equal 0, but in this library's case equals 1.\n",
    "\n",
    "Keeping in mind that it is named distance but really is similarity, here are some examples:\n",
    "\n",
    "```\n",
    "improving performance in neural networks using a boosting algorithm. \n",
    "     vs \n",
    "improving performance in neural networks using a boosting algorithm.\n",
    "sim: 1.0\n",
    "\n",
    "improving performance in neural networks using a boosting algorithm. \n",
    "     vs \n",
    "inference of finite automata using homing sequences.\n",
    "sim: 0.75\n",
    "```\n",
    "\n",
    "This is weird if you consider the following:\n",
    "\n",
    "```\n",
    ">>> print distance.get_jaro_distance(\"hello\", \"haloa\", winkler=True, scaling=0.1)\n",
    "0.76\n",
    "```\n",
    "\n",
    "Adding noise improves the similarity :/\n",
    "\n",
    "```\n",
    "improving performance in neural networks using a boosting algorithm. \n",
    "     vs \n",
    "inference of finite automata using homing sequences. in r.l.\n",
    "sim: 0.78\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### But that's OK.\n",
    "\n",
    "All literature makes mention of the fact that there is a step where you have to decide a threshold for any similarity metric, above which things can be considered coreferent. Perhaps that threshold is really high if the JW distance is the metric used.\n",
    "\n",
    "At this point, we have a couple options:\n",
    "1. Discretize the output of the get_jaro_distance function, esp. for publication titles by choosing a threshold as others do\n",
    "2. Use a soft TF-IDF distance measure for titles as others do\n",
    "\n",
    "Question: Can PSL do anything about this? What comes to mind is lowering the weight of the rule, but that's being unfair to the rule, in my opinion, if you want to maintain the semantics. Increasing the negative prior? I don't know, makes it feel like a hack to make allowance for something that isn't really doing *its* job well (I mean the distance metric)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using TFIDF \n",
    "\n",
    "```\n",
    "improving performance in neural networks using a boosting algorithm. \n",
    "     vs \n",
    "inference of finite automata using homing sequences.\n",
    "sim: 0.0818\n",
    "\n",
    "improving performance in neural networks using a boosting algorithm. \n",
    "     vs \n",
    "improving performance in neural networks using a boosting algorithm.\n",
    "sim: 1.0\n",
    "\n",
    "improving performance in neural networks using a boosting algorithm. \n",
    "     vs \n",
    "inference of finite automata using homing sequences. in r.l.\n",
    "sim: 0.1472\n",
    "```\n",
    "\n",
    "Much better, what? Why adding in r.l. should improve similarity, I don't understand. But oh well. It's acceptably low still."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Did the results improve?\n",
    "\n",
    "#### Of course they did.\n",
    "\n",
    "```\n",
    "Without Trans:\n",
    "\n",
    "Stats for Author\n",
    "Positive Class: P 0.8577, R 0.8139\n",
    "Negative Class: P 0.9797, R 0.9852\n",
    "\n",
    "Stats for Publications\n",
    "Positive Class: P 0.9589, R 0.7955\n",
    "Negative Class: P 0.9963, R 0.9994\n",
    "```\n",
    "\n",
    "Notice that the recall went down a few points though\n",
    "\n",
    "```\n",
    "With Trans:\n",
    "\n",
    "Stats for Author\n",
    "Positive Class: P 0.8571, R 0.9419\n",
    "Negative Class: P 0.9936, R 0.9828\n",
    "\n",
    "Stats for Publications\n",
    "Positive Class: P 0.9589, R 0.7955\n",
    "Negative Class: P 0.9963, R 0.9994\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transitivity seems to be doing nothing for publications, it would seem based on these results..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
